{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "avito_pipeline.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "mount_file_id": "1_allu-WkIXygzJIld61GCiqpy8Xopy5R",
   "authorship_tag": "ABX9TyNLZO+idENembXdNwYdzK/2"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qqwjnfXEEEsI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1638531613186,
     "user_tz": -180,
     "elapsed": 3353,
     "user": {
      "displayName": "Данил Ельцов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01985227269587903507"
     }
    },
    "outputId": "1d3c743f-6830-41d8-a8c7-3388fb2a0121",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "!cp -f /content/drive/MyDrive/ml/utils/utils.py ./\n",
    "!wget http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
    "!tar -xvf mystem-3.0-linux3.1-64bit.tar.gz\n",
    "!mkdir -p /root/.local/bin\n",
    "!cp -f mystem /root/.local/bin/mystem"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2021-12-03 11:40:09--  http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
      "Resolving download.cdn.yandex.net (download.cdn.yandex.net)... 5.45.205.241, 5.45.205.244, 5.45.205.245, ...\n",
      "Connecting to download.cdn.yandex.net (download.cdn.yandex.net)|5.45.205.241|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: http://cache-man01i.cdn.yandex.net/download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz [following]\n",
      "--2021-12-03 11:40:10--  http://cache-man01i.cdn.yandex.net/download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
      "Resolving cache-man01i.cdn.yandex.net (cache-man01i.cdn.yandex.net)... 5.45.205.221, 2a02:6b8::3:221\n",
      "Connecting to cache-man01i.cdn.yandex.net (cache-man01i.cdn.yandex.net)|5.45.205.221|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16457938 (16M) [application/octet-stream]\n",
      "Saving to: ‘mystem-3.0-linux3.1-64bit.tar.gz.2’\n",
      "\n",
      "mystem-3.0-linux3.1 100%[===================>]  15.70M  9.81MB/s    in 1.6s    \n",
      "\n",
      "2021-12-03 11:40:12 (9.81 MB/s) - ‘mystem-3.0-linux3.1-64bit.tar.gz.2’ saved [16457938/16457938]\n",
      "\n",
      "mystem\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "34CUrLaJE_gT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "!pip3 install "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "epjZUJ8rD9BO",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1638532587654,
     "user_tz": -180,
     "elapsed": 237,
     "user": {
      "displayName": "Данил Ельцов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01985227269587903507"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import gc\n",
    "import glob\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from utils import *"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uLMO677wFYoQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1638531702080,
     "user_tz": -180,
     "elapsed": 227,
     "user": {
      "displayName": "Данил Ельцов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01985227269587903507"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# paths\n",
    "ROOT, ROOT_DATA, ROOT_MODELS, ROOT_GRAPHS, ROOT_IMAGES = get_default_paths('/content/drive/MyDrive/ml/hacks/avito')\n",
    "\n",
    "# images\n",
    "IMG_HEIGHT, IMG_WIDTH = 224, 224\n",
    "image_batch_size = 100\n",
    "model_link = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5\"\n",
    "image_prefix = \"image_\"\n",
    "\n",
    "# text\n",
    "title_batch_size = 10000\n",
    "title_vector_size = 100\n",
    "title_window_size = 5\n",
    "description_batch_size = 10000\n",
    "description_vector_size = 300\n",
    "description_window_size = 10\n",
    "bert_prefix = \"bert_\"\n",
    "doc2vec_prefix = \"doc2vec_\"\n",
    "doc2vec_workers = 2\n",
    "doc2vec_epochs = 30\n",
    "\n",
    "# tabnet\n",
    "tabnet_batch_size = 1000\n",
    "tabnet_shuffle_size = 10000\n",
    "tabnet_epochs = 15\n",
    "tabnet_lr = 1e-4\n",
    "tabnet_layers = 5\n",
    "tabnet_steps_epoch = 10000\n",
    "tabnet_valid_steps = 1000\n",
    "tabnet_feature_relaxation = 1.2\n",
    "tabnet_sparsity_coeff = 1e-3\n",
    "tabnet_batch_momentum = 0.8\n",
    "tabnet_virtual_batch_size = 250\n",
    "tabnet_feature_dim = 64\n",
    "tabnet_output_dim = 32\n",
    "\n",
    "# general\n",
    "train_size = int(1.5e6)\n",
    "valid_size = train_size * 0.3\n",
    "batch_size = 10000\n",
    "categorical_threshold = 100\n",
    "\n",
    "# columns\n",
    "categorical_columns = ['region', 'city', 'parent_category_name', 'category_name', 'param_1', 'param_2', 'param_3',\n",
    "                       'user_type', 'item_seq_number']\n",
    "gen_categorical_columns = [col + \"_\" for col in categorical_columns]\n",
    "numerical_columns = ['image_top_1', 'price']\n",
    "gen_numerical_columns = ['price_mean', 'price_std']\n",
    "text_columns = ['title', 'description']\n",
    "meta_columns = ['item_id', 'user_id', 'image', 'activation_date']\n",
    "label = 'deal_probability'\n",
    "feature_columns = numerical_columns + gen_numerical_columns + categorical_columns + gen_categorical_columns \\\n",
    "                  + [bert_prefix, doc2vec_prefix, image_prefix]\n",
    "\n",
    "# set pretty output for pandas dataframes\n",
    "prettify_pandas_print()"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "A4laoZjVFW8C",
    "executionInfo": {
     "status": "error",
     "timestamp": 1638532618447,
     "user_tz": -180,
     "elapsed": 28705,
     "user": {
      "displayName": "Данил Ельцов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01985227269587903507"
     }
    },
    "outputId": "167e6f04-b7ae-430f-afd8-a8673744226e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def make_features():\n",
    "    # load train data and sort it for avoiding future leak\n",
    "    train = pd.read_csv(f'{ROOT_DATA}/train.csv').sort_values('activation_date')\n",
    "    train['is_train'] = '1'\n",
    "\n",
    "    test = pd.read_csv(f'{ROOT_DATA}/test.csv')\n",
    "    test['is_train'] = '0'\n",
    "\n",
    "    df = pd.concat([train, test], axis=0)\n",
    "\n",
    "    # price has lognormal distribution\n",
    "    df = log_scale(df, 'price', 0, 1e6)\n",
    "    df = fill_mean_by(df, 'price', 'parent_category_name', drop_mean=False)\n",
    "    df = add_std_by(df, 'price', 'parent_category_name')\n",
    "    df = fill_value(df, 'image_top_1', 0)\n",
    "\n",
    "    # clean and lemmatize russian text\n",
    "    df = process_russian_cols(df, text_columns)\n",
    "\n",
    "    # create ohe for category\n",
    "    for col in categorical_columns:\n",
    "        df = ohe_col_with_threshold(df, col, categorical_threshold)\n",
    "\n",
    "    df[df['is_train'] == '1'].to_csv(f'{ROOT_DATA}/train_features.csv', index=False, header=True)\n",
    "    df[df['is_train'] == '0'].to_csv(f'{ROOT_DATA}/test_features.csv', index=False, header=True)\n",
    "\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "# making base features for train/test\n",
    "make_features()"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32mpandas/_libs/hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.map_locations\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: unhashable type: 'list'"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Exception ignored in: 'pandas._libs.index.IndexEngine._call_map_locations'\n",
      "Traceback (most recent call last):\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1709, in pandas._libs.hashtable.PyObjectHashTable.map_locations\n",
      "TypeError: unhashable type: 'list'\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidIndexError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001B[0m in \u001B[0;36mintersection\u001B[0;34m(self, other, sort)\u001B[0m\n\u001B[1;32m   2677\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2678\u001B[0;31m             \u001B[0mindexer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mIndex\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrvals\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_indexer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlvals\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2679\u001B[0m             \u001B[0mindexer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mindexer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtake\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindexer\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnonzero\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001B[0m in \u001B[0;36mget_indexer\u001B[0;34m(self, target, method, limit, tolerance)\u001B[0m\n\u001B[1;32m   2989\u001B[0m             raise InvalidIndexError(\n\u001B[0;32m-> 2990\u001B[0;31m                 \u001B[0;34m\"Reindexing only valid with uniquely valued Index objects\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2991\u001B[0m             )\n",
      "\u001B[0;31mInvalidIndexError\u001B[0m: Reindexing only valid with uniquely valued Index objects",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32mpandas/_libs/hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.map_locations\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: unhashable type: 'list'"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Exception ignored in: 'pandas._libs.index.IndexEngine._call_map_locations'\n",
      "Traceback (most recent call last):\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1709, in pandas._libs.hashtable.PyObjectHashTable.map_locations\n",
      "TypeError: unhashable type: 'list'\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidIndexError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001B[0m in \u001B[0;36mintersection\u001B[0;34m(self, other, sort)\u001B[0m\n\u001B[1;32m   2677\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2678\u001B[0;31m             \u001B[0mindexer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mIndex\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrvals\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_indexer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlvals\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2679\u001B[0m             \u001B[0mindexer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mindexer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtake\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindexer\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnonzero\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001B[0m in \u001B[0;36mget_indexer\u001B[0;34m(self, target, method, limit, tolerance)\u001B[0m\n\u001B[1;32m   2989\u001B[0m             raise InvalidIndexError(\n\u001B[0;32m-> 2990\u001B[0;31m                 \u001B[0;34m\"Reindexing only valid with uniquely valued Index objects\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2991\u001B[0m             )\n",
      "\u001B[0;31mInvalidIndexError\u001B[0m: Reindexing only valid with uniquely valued Index objects",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-17-6ffd4ac52098>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[0;31m# making base features for train/test\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 32\u001B[0;31m \u001B[0mmake_features\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-17-6ffd4ac52098>\u001B[0m in \u001B[0;36mmake_features\u001B[0;34m()\u001B[0m\n\u001B[1;32m     11\u001B[0m     \u001B[0;31m# price has lognormal distribution\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m     \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlog_scale\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'price'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1e6\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 13\u001B[0;31m     \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfill_mean_by\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'price'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'parent_category_name'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdrop_mean\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     14\u001B[0m     \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0madd_std_by\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'price'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'parent_category_name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m     \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfill_value\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'image_top_1'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/content/utils.py\u001B[0m in \u001B[0;36mfill_mean_by\u001B[0;34m(df, col, by, drop_mean)\u001B[0m\n\u001B[1;32m     97\u001B[0m \u001B[0;31m# fill empty column values by mean grouping with by\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mfill_mean_by\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mby\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdrop_mean\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 99\u001B[0;31m     \u001B[0mmean_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0madd_mean_by\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mby\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    100\u001B[0m     \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmerge\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmean_df\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    101\u001B[0m     \u001B[0mdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfillna\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'{}_mean'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/content/utils.py\u001B[0m in \u001B[0;36madd_mean_by\u001B[0;34m(df, col, by)\u001B[0m\n\u001B[1;32m     81\u001B[0m     \u001B[0mmean_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0magg_vals\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'mean'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreset_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     82\u001B[0m     \u001B[0mmean_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mby\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'{}_mean'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 83\u001B[0;31m     \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmerge\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmean_df\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     84\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     85\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001B[0m in \u001B[0;36mmerge\u001B[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001B[0m\n\u001B[1;32m   7961\u001B[0m             \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   7962\u001B[0m             \u001B[0mindicator\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mindicator\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 7963\u001B[0;31m             \u001B[0mvalidate\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mvalidate\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   7964\u001B[0m         )\n\u001B[1;32m   7965\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/reshape/merge.py\u001B[0m in \u001B[0;36mmerge\u001B[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001B[0m\n\u001B[1;32m     85\u001B[0m         \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     86\u001B[0m         \u001B[0mindicator\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mindicator\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 87\u001B[0;31m         \u001B[0mvalidate\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mvalidate\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     88\u001B[0m     )\n\u001B[1;32m     89\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mop\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_result\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/reshape/merge.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001B[0m\n\u001B[1;32m    643\u001B[0m             \u001B[0mwarnings\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwarn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mUserWarning\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    644\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 645\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_validate_specification\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    646\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    647\u001B[0m         \u001B[0;31m# note this function has side effects\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/reshape/merge.py\u001B[0m in \u001B[0;36m_validate_specification\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1212\u001B[0m                 \u001B[0mleft_cols\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mleft\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1213\u001B[0m                 \u001B[0mright_cols\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mright\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1214\u001B[0;31m                 \u001B[0mcommon_cols\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mleft_cols\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mintersection\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mright_cols\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1215\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommon_cols\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1216\u001B[0m                     raise MergeError(\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001B[0m in \u001B[0;36mintersection\u001B[0;34m(self, other, sort)\u001B[0m\n\u001B[1;32m   2681\u001B[0m             \u001B[0;31m# InvalidIndexError raised by get_indexer if non-unique\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2682\u001B[0m             \u001B[0;31m# IncompatibleFrequency raised by PeriodIndex.get_indexer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2683\u001B[0;31m             \u001B[0mindexer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0malgos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munique1d\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mIndex\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrvals\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_indexer_non_unique\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlvals\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2684\u001B[0m             \u001B[0mindexer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mindexer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mindexer\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2685\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001B[0m in \u001B[0;36mget_indexer_non_unique\u001B[0;34m(self, target)\u001B[0m\n\u001B[1;32m   4698\u001B[0m             \u001B[0mtgt_values\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_engine_target\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4699\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 4700\u001B[0;31m         \u001B[0mindexer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmissing\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_indexer_non_unique\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtgt_values\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   4701\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mensure_platform_int\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindexer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmissing\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4702\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_indexer_non_unique\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: unhashable type: 'list'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xeeYsH-MFUtk",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# preparing for text\n",
    "def get_chunks(name):\n",
    "    return pd.read_csv(f'{ROOT_DATA}/{name}_features.csv', chunksize=batch_size)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NvgF0JbKFOlx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "build_doc2vec(df=get_chunks('train'), col='title', chunked=True, fill_na='продать', workers=doc2vec_workers,\n",
    "              tmp_path=ROOT_MODELS, vector_size=title_vector_size, window_size=title_window_size, epochs=doc2vec_epochs)\n",
    "build_doc2vec(df=get_chunks('train'), col='description', chunked=True, fill_na='продать', workers=doc2vec_workers,\n",
    "              tmp_path=ROOT_MODELS, vector_size=description_vector_size, window_size=description_window_size,\n",
    "              epochs=doc2vec_epochs)\n",
    "\n",
    "bert_model, bert_tokenizer = get_tiny_bert_model()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jvsL64Y3FMwc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# preparing for images\n",
    "create_empty_image(IMG_WIDTH, IMG_HEIGHT, f\"{ROOT_IMAGES}/empty.jpg\")\n",
    "image_paths = {image_path.split(\"/\")[-1].replace(\".jpg\", \"\"): image_path for image_path in\n",
    "               glob.glob(f\"{ROOT_IMAGES}/*.jpg\")}\n",
    "image_model = create_image_model(IMG_HEIGHT, IMG_WIDTH, model_link)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QlQYaGARFJtx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def add_embeddings(name):\n",
    "    first_valid = True\n",
    "    for i, chunk in enumerate(get_chunks(name)):\n",
    "        chunk = add_tiny_bert_embeds(chunk, 'title', fillna='продать', model=bert_model, tokenizer=bert_tokenizer,\n",
    "                                     pref=bert_prefix)\n",
    "        chunk = add_doc2vec_embeds(chunk, 'title', fillna='продать', model_path=f'{ROOT_MODELS}/doc2vec_title',\n",
    "                                   pref=doc2vec_prefix)\n",
    "        chunk = add_tiny_bert_embeds(chunk, 'description', fillna='продать', model=bert_model, tokenizer=bert_tokenizer,\n",
    "                                     pref=bert_prefix)\n",
    "        chunk = add_doc2vec_embeds(chunk, 'description', fillna='продать',\n",
    "                                   model_path=f'{ROOT_MODELS}/doc2vec_description',\n",
    "                                   pref=doc2vec_prefix)\n",
    "        chunk = images_embedding(chunk, 'image', paths=image_paths, w=IMG_HEIGHT, h=IMG_WIDTH, model=image_model,\n",
    "                                 pref=image_prefix, batch_size=image_batch_size)\n",
    "\n",
    "        is_valid = i * chunk >= (train_size - valid_size) and name != 'test'\n",
    "        header = (i == 0) or (is_valid and first_valid)\n",
    "        mode = 'w' if (i == 0) or (is_valid and first_valid) else 'a'\n",
    "\n",
    "        if is_valid and first_valid:\n",
    "            first_valid = False\n",
    "\n",
    "        if not is_valid:\n",
    "            chunk.to_csv(f'{ROOT_DATA}/{name}_dataset.csv', index=False, header=header, mode=mode)\n",
    "        else:\n",
    "            chunk.to_csv(f'{ROOT_DATA}/valid_dataset.csv', index=False, header=header, mode=mode)\n",
    "\n",
    "\n",
    "add_embeddings('train')\n",
    "add_embeddings('test')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-25gKLfMFFG_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def create_tabnet_model(feature_columns):\n",
    "    import tensorflow as tf\n",
    "    import tabnet\n",
    "    train = get_tensorflow_dataset(f'{ROOT_DATA}/train_dataset.csv', batch_size=tabnet_batch_size,\n",
    "                                   label=label,\n",
    "                                   feature_columns=feature_columns,\n",
    "                                   shuffle_buffer_size=tabnet_shuffle_size).prefetch(10)\n",
    "    valid = get_tensorflow_dataset(f'{ROOT_DATA}/valid_dataset.csv', batch_size=tabnet_batch_size,\n",
    "                                   label=label,\n",
    "                                   feature_columns=feature_columns,\n",
    "                                   shuffle_buffer_size=tabnet_shuffle_size).prefetch(10)\n",
    "    # Use Group Normalization for small batch sizes\n",
    "    model = tabnet.TabNetRegressor(feature_columns=None,\n",
    "                                   num_regressors=1,\n",
    "                                   num_decision_steps=tabnet_layers,\n",
    "                                   relaxation_factor=tabnet_feature_relaxation,\n",
    "                                   sparsity_coefficient=tabnet_sparsity_coeff,\n",
    "                                   batch_momentum=tabnet_batch_momentum,\n",
    "                                   virtual_batch_size=tabnet_virtual_batch_size,\n",
    "                                   feature_dim=tabnet_feature_dim,\n",
    "                                   output_dim=tabnet_output_dim)\n",
    "\n",
    "    lr = tf.keras.optimizers.schedules.ExponentialDecay(tabnet_lr, decay_steps=2000, decay_rate=0.95, staircase=False)\n",
    "    optimizer = tf.keras.optimizers.Adam(lr)\n",
    "    model.compile(optimizer, loss='mse', metrics=['mse', 'mae'])\n",
    "\n",
    "    model.fit(train, epochs=tabnet_epochs, validation_data=valid, verbose=True, steps_per_epoch=tabnet_steps_epoch,\n",
    "              validation_steps=tabnet_valid_steps)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "tabnet_model = create_tabnet_model(feature_columns)\n",
    "tabnet_model.save(f'{ROOT_MODELS}/tabnet')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QGQ4p2JUFDWm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "test_chunks = pd.read_csv(f'{ROOT_DATA}/test_dataset.csv', chunksize=batch_size)\n",
    "for i, test_chunk in enumerate(test_chunks):\n",
    "    features = [col for col in test_chunk.columns if col.startswith(feature_columns)]\n",
    "    test_chunk[label] = pd.Series(tabnet_model(tf.convert_to_tensor(test_chunks[features].values)).numpy())\n",
    "    test_chunks[['item_id', label]].to_csv(f'{ROOT_DATA}/tabnet_submission.csv', index=False, header=i == 0,\n",
    "                                           mode='w' if i == 0 else 'a')"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}