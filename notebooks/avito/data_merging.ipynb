{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "data_merging.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "ojY1-iSK7ESe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "!pip install pandas"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3JZvqjCM610n",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import gc"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PMOMbQuV8SH6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "!wc -l /storage/data/simple_features.csv"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wxoKgojN82Eu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "total_items = 1590000\n",
    "validation_split = 0.3\n",
    "data_chunksize = 100_000\n",
    "text_chunksize = 10_000\n",
    "images_chunksize = 10_000"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XHQDxikUH0u-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def fill_empty_images(row, empty_image):\n",
    "  row = row.to_dict()\n",
    "  if row['_merge'] == 'left_only':\n",
    "    row.update(empty_image)\n",
    "  return pd.Series(row)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jKZmqXab7Kc_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "data_chunks = pd.read_csv('/storage/data/simple_features.csv', chunksize=data_chunksize)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Lx5ZqIXW8BsY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# as data was already presorted by date so we can just mark first rows for train and last rows for validation\n",
    "for i, data_chunk in enumerate(data_chunks):\n",
    "  test_chunk = total_items * (1 - validation_split) < i * data_chunksize\n",
    "  indexes = data_chunk['item_id'].tolist()\n",
    "\n",
    "  text_chunks = pd.read_csv('/storage/data/text_train.csv', chunksize=text_chunksize)\n",
    "  dataframes = []\n",
    "  for j, text_chunk in enumerate(text_chunks):\n",
    "    dataframes.append(text_chunk[text_chunk['item_id'].isin(indexes)])\n",
    "  data_chunk = data_chunk.merge(pd.concat(dataframes, axis=0), on='item_id', how='left')\n",
    "  dataframes = None\n",
    "  gc.collect()\n",
    "\n",
    "  images_chunks = pd.read_csv('/storage/data/images_resnet.csv', names=['image_id'] + ['image_{}'.format(i) for i in range(2048)],\n",
    "                              chunksize=images_chunksize)\n",
    "  image_indexes = data_chunk['image'].tolist()\n",
    "  dataframes = []\n",
    "  for j, image_chunk in enumerate(images_chunks):\n",
    "    dataframes.append(image_chunk[image_chunk['image_id'].isin(image_indexes)])\n",
    "    empty_image_df = image_chunk[image_chunk['image_id'] == 'empty']\n",
    "    if len(empty_image_df) > 0:\n",
    "      empty_image = empty_image_df.iloc[0].to_dict()\n",
    "  data_chunk = data_chunk.merge(pd.concat(dataframes, axis=0), \n",
    "                                left_on='image', right_on='image_id', how='left', indicator='_merge')\n",
    "  data_chunk = data_chunk.apply(lambda row: fill_empty_images(row, empty_image), axis=1)\n",
    "  data_chunk = data_chunk.drop('_merge', axis=1)\n",
    "  dataframes = None\n",
    "\n",
    "  n_file = i == 0 or (test_chunk and total_items * (1 - validation_split) >= (i-1) * data_chunksize)\n",
    "  data_chunk.to_csv('/storage/data/dataset_{}'.format('valid' if test_chunk else 'train'), \n",
    "                    header = n_file, index=False, mode = 'w' if n_file else 'a')\n",
    "  data_chunk = None\n",
    "  gc.collect()\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}